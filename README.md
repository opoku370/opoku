# DATA ANALYST

## ABOUT ME 
--------------
Results-driven Data Analyst with 3+ years of experience in data analytics, information technology support, business intelligence, and process optimization, combined with a strong background in Quantum Espresso computational chemistry. Highly skilled in leveraging SQL, data modelling, and BI tools to translate complex data into actionable business insights. Proven ability to enhance decision-making and operational efficiency across sectors, including healthcare data management. Highly passionate about solving business challenges through data-driven solutions, with excellent communication skills to bridge technical and non-technical audiences effectively.

## EDUCATION
----------------
Berlin School of Business and Innovation

Master’s Degree, Data Analytics

October 2024 – April 2026

-------
Kwame Nkrumah University of Science and Technology

Bachelor’s Degree, Chemistry

September 2017 – November 2021



## PROJECTS
----------------

[Data Co Supply Chain Analysis](https://opoku370.github.io/supply-chain-analysis/ )

[Tableu Project on Data Co Supply Chain](https://public.tableau.com/app/profile/emmanuel.opoku3814/viz/SupplyChainTableauDashboard/SalesDashboard?publish=yes )

[Predicting Diabetes from Basic Health Data: A Machine Learning Approach ](https://opoku370.github.io/diabetes_predictive_analysis/)



## SKILLS
------------

Python, Machine learning Models ,Data Visualization, SQL, Tableau, Microsoft Excel, Big Data Technologies (Hadoop, Spark, Kafka, Flink, Storm), NoSQL Databases (MongoDB, Cassandra), Distributed Computing (MapReduce, Streaming), Data Warehousing (Hive, Google Cloud), Data Architecture (Lambda, Kappa),Insights, Reports, Problem solver, Critical thinking, Team goal Oriented and collaboration.



EXPERTISE TECHNOLOGIES
--------------

Python:
-	Pandas: Data manipulation and analysis
-	NumPy: Numerical Computations
-	Matplotlib & Seaborn: Data Visualizations
-	Jupyter Notebook: Interactive development and data Analysis environment
- Model Selection and Training: Supervised and unsupervised algorithms such as Linear/Logistic Regression, Decision Trees, Random Forest, K-Nearest Neighbors (KNN), Naïve Bayes, Support Vector Machines (SVM), Gradient Boosting (e.g., XGBoost, and Clustering (K-Means).
- Model Evaluation: Metrics such as Accuracy, Precision, Recall, F1-Score, AUC-ROC, Confusion Matrix, MAE, and RMSE to ensure robustness and generalizability.


Tableau:

-	Calculated Fields: Creating complex calculated metrics
-	Data Blending and Joins: Merging data from multiple sources and managing relationships using joins and blends
-	Tableau Public Publishing: Publishing and managing dashboards for sharing and collaboration.
-	Interactive Dashboards Development: Building Dynamic Dashboards with filters, parameters and actions for user friendly experiences.

Microsoft Excel:

-	Power Query: Data connection and transformation
-	Power Pivot: Data modelling for large datasets
-	VBA (Visual Basic for Applications): Automating repetitive tasks and creating micros.
-	PivotTables and PivotCharts: Data summarisation and analysis.


Database Management Systems:

-	Relational Database Systems (RDBMS):
Proficient in PostgreSQL, MySQL, Microsoft SQL Server, and Oracle DB for designing, querying, and managing structured data.
-	SQL Programming:
Proficient in writing optimized SQL queries, including complex joins, subqueries, window functions, CTEs, and aggregate functions for data manipulation and reporting.
-	Database Design & Normalization:
Skilled in designing normalized database schemas using ER diagrams, applying 1NF, 2NF, and 3NF to ensure data integrity and eliminate redundancy.
-	Stored Procedures, Triggers, and Functions:
Expertise in developing stored procedures, user-defined functions, and triggers to automate business logic and enforce rules within the database.
-	Transaction Management & ACID Properties:
In-depth understanding of ACID principles (Atomicity, Consistency, Isolation, Durability) for managing secure and reliable database transactions.
-	NoSQL Databases:
Familiar with MongoDB,  and Cassandra for handling unstructured or semi-structured data.
-	New SQL Databases:
Understanding of modern NewSQL databases like Google Spanner, and CockroachDB that combine the scalability of NoSQL systems with the ACID compliance of traditional RDBMS.
-	Data Backup & Recovery:
Experience in implementing backup strategies, automated scheduled backups, and recovery plans to ensure business continuity.
-	Database Migration & Integration:
Expertise in migrating data between systems using ETL tools, SQL scripts, and data import/export utilities across platforms (on-premise and cloud).
-	Cloud Databases & Services:
Practical knowledge of cloud-hosted databases like Google Cloud SQL for scalable and highly available solutions.
-	Monitoring & Administration Tools:
Proficient with tools like pgAdmin, SQL Server Management Studio (SSMS), MySQL Workbench, and Oracle SQL Developer for database administration and visualization.


Big Data Analytics:

-	Big Data Architectures (Lambda and Kappa):
  -	Understanding of Lambda Architecture to process massive data volumes with a combination of batch (Hadoop/Spark) and real-time (Kafka/Storm) processing layers.
  -	Understanding of Kappa Architecture for handling streaming data pipelines where all processing is done in real-time using systems like Kafka, Flink, and Spark Streaming.
-	Apache Hadoop Ecosystem:
Extensive experience with Apache Hadoop for distributed storage (HDFS) and large-scale batch data processing, using the MapReduce programming model for scalable computation.
-	Apache Spark:
Skilled in Apache Spark for in-memory distributed data processing, real-time analytics, machine learning (Spark MLlib), and stream processing for large datasets.
-	Apache Hive:
Proficient in using Hive for querying and managing large structured datasets in distributed storage using SQL-like queries, enabling efficient data warehousing over Hadoop.
-	Apache Kafka:
Expertise in Apache Kafka for building real-time data pipelines, stream processing, and event-driven architectures, supporting high-throughput data ingestion.
-	Apache Storm and Apache Flink:
Experience with Apache Storm for real-time, fault-tolerant, and distributed stream processing, and Apache Flink for advanced stateful stream and batch processing with low-latency data flows.
-	NoSQL Databases (MongoDB and Apache Cassandra):
Hands-on expertise in MongoDB (document-oriented storage) and Apache Cassandra (wide-column store) for managing high-volume, semi-structured, and unstructured data with a focus on horizontal scalability and high availability.
-	Apache Pig:
Proficient in using Apache Pig for analyzing large datasets through high-level scripting (Pig Latin) and automating complex data transformations on Hadoop clusters.
-	Resource Management with YARN:
Strong understanding of Yet Another Resource Negotiator (YARN) for resource management and job scheduling across Hadoop clusters, ensuring optimal use of computational resources.
-	Data Warehousing and Data Lakes:
Proficient in developing modern data warehousing solutions and implementing data lakes using cloud platforms such as Google Cloud (BigQuery, Cloud Storage), enabling flexible storage, real-time analytics, and advanced machine learning integration.



CONTACTS
-----------------
emmanuelopoku024578@gmail.com














